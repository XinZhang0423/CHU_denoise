{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "# 模型相关\n",
    "import torch\n",
    "from models.DIP_2D import DIP_2D\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "\n",
    "# 画图相关\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 文件读写相关\n",
    "import csv\n",
    "from config import *\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# 自定义函数\n",
    "from utils.pre_utils import *\n",
    "\n",
    "import datetime\n",
    "print(config['sub_iter_DIP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一个test，用于还原DIP paper中的那个用噪音文件学习不同图片的那个学习曲线learning_curve\n",
    "# 输入是高斯噪音，目标分别是 ground_truth，含噪音图片，纯噪音(应该用另一个噪音的干！)\n",
    "test_list = [\"images/noise_images/ground_truth.npy\",\"images/noise_images/BSREM_it30.npy\",\"images/noise_images/uniform_noise2.npy\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "staaaaaaaaaaand\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# input 这里我觉得可以测试不同的先测试一下高斯噪声吧，我觉得不同的噪声有可能对于学习曲线有影响,读取并标准化input\n",
    "image_net_input = np.load(\"images/noise_images/uniform_noise.npy\")\n",
    "image_net_input_scale,param1_scale_im_net,param2_scale_im_net = rescale_imag(image_net_input,\"standardization\") \n",
    "image_net_input_torch = torch.Tensor(image_net_input_scale)\n",
    "image_net_input_torch = image_net_input_torch.view(1,1,PETImage_shape[0],PETImage_shape[1],PETImage_shape[2])\n",
    "image_net_input_torch = image_net_input_torch[:,:,:,:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/xzhang/miniconda3/envs/dip/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1292: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name       | Type       | Params\n",
      "-------------------------------------------\n",
      "0  | deep1      | Sequential | 2.5 K \n",
      "1  | down1      | Sequential | 2.4 K \n",
      "2  | deep2      | Sequential | 14.0 K\n",
      "3  | down2      | Sequential | 9.3 K \n",
      "4  | deep3      | Sequential | 55.7 K\n",
      "5  | down3      | Sequential | 37.1 K\n",
      "6  | deep4      | Sequential | 221 K \n",
      "7  | up1        | Sequential | 73.9 K\n",
      "8  | deep5      | Sequential | 74.1 K\n",
      "9  | up2        | Sequential | 18.5 K\n",
      "10 | deep6      | Sequential | 18.6 K\n",
      "11 | up3        | Sequential | 4.7 K \n",
      "12 | deep7      | Sequential | 2.5 K \n",
      "13 | positivity | ReLU       | 0     \n",
      "-------------------------------------------\n",
      "535 K     Trainable params\n",
      "0         Non-trainable params\n",
      "535 K     Total params\n",
      "2.141     Total estimated model params size (MB)\n",
      "/home/xzhang/miniconda3/envs/dip/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/xzhang/miniconda3/envs/dip/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:322: UserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/noise_images/ground_truth.npy\n",
      "staaaaaaaaaaand\n",
      "10000\n",
      "Epoch 9999: 100%|██████████| 1/1 [00:00<00:00, 20.95it/s, loss=0.000157, v_num=0]   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "   | Name       | Type       | Params\n",
      "-------------------------------------------\n",
      "0  | deep1      | Sequential | 2.5 K \n",
      "1  | down1      | Sequential | 2.4 K \n",
      "2  | deep2      | Sequential | 14.0 K\n",
      "3  | down2      | Sequential | 9.3 K \n",
      "4  | deep3      | Sequential | 55.7 K\n",
      "5  | down3      | Sequential | 37.1 K\n",
      "6  | deep4      | Sequential | 221 K \n",
      "7  | up1        | Sequential | 73.9 K\n",
      "8  | deep5      | Sequential | 74.1 K\n",
      "9  | up2        | Sequential | 18.5 K\n",
      "10 | deep6      | Sequential | 18.6 K\n",
      "11 | up3        | Sequential | 4.7 K \n",
      "12 | deep7      | Sequential | 2.5 K \n",
      "13 | positivity | ReLU       | 0     \n",
      "-------------------------------------------\n",
      "535 K     Trainable params\n",
      "0         Non-trainable params\n",
      "535 K     Total params\n",
      "2.141     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/noise_images/BSREM_it30.npy\n",
      "staaaaaaaaaaand\n",
      "10000\n",
      "Epoch 9999: 100%|██████████| 1/1 [00:00<00:00, 19.40it/s, loss=0.00118, v_num=0]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "   | Name       | Type       | Params\n",
      "-------------------------------------------\n",
      "0  | deep1      | Sequential | 2.5 K \n",
      "1  | down1      | Sequential | 2.4 K \n",
      "2  | deep2      | Sequential | 14.0 K\n",
      "3  | down2      | Sequential | 9.3 K \n",
      "4  | deep3      | Sequential | 55.7 K\n",
      "5  | down3      | Sequential | 37.1 K\n",
      "6  | deep4      | Sequential | 221 K \n",
      "7  | up1        | Sequential | 73.9 K\n",
      "8  | deep5      | Sequential | 74.1 K\n",
      "9  | up2        | Sequential | 18.5 K\n",
      "10 | deep6      | Sequential | 18.6 K\n",
      "11 | up3        | Sequential | 4.7 K \n",
      "12 | deep7      | Sequential | 2.5 K \n",
      "13 | positivity | ReLU       | 0     \n",
      "-------------------------------------------\n",
      "535 K     Trainable params\n",
      "0         Non-trainable params\n",
      "535 K     Total params\n",
      "2.141     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "images/noise_images/uniform_noise2.npy\n",
      "staaaaaaaaaaand\n",
      "10000\n",
      "Epoch 9999: 100%|██████████| 1/1 [00:00<00:00, 20.66it/s, loss=0.0038, v_num=0]     \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 读取三个含噪声文件，将其做rescale和格式转换tensor 1,1,112,112\n",
    "ground_truth = np.load(\"/home/xzhang/Documents/我的模型/images/noise_images/ground_truth.npy\")\n",
    "\n",
    "for path_noisy in test_list:\n",
    "    print(path_noisy)\n",
    "    image_corrupt=np.load(path_noisy) # 读取图片并将图片转换成numpy array\n",
    "    image_corrupt_input_scaled,param1_scale_im_corrupt,param2_scale_im_corrupt = rescale_imag(image_corrupt,\"standardization\") # 标准化图片, 减去平均值，除以标准差，参数1是mean，参数2是std\n",
    "    image_corrupt_torch = torch.Tensor(image_corrupt_input_scaled)\n",
    "    image_corrupt_torch = image_corrupt_torch.view(1,1,PETImage_shape[0],PETImage_shape[1],PETImage_shape[2])\n",
    "    image_corrupt_torch = image_corrupt_torch[:,:,:,:,0]\n",
    "\n",
    "    # 用dataset和dataloader 读取训练数据 input和目标\n",
    "    train_dataset = torch.utils.data.TensorDataset(image_net_input_torch, image_corrupt_torch)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1) \n",
    "\n",
    "    # 读取并导入模型\n",
    "    model = DIP_2D(param1_scale_im_corrupt, param2_scale_im_corrupt,config,'data/Algo/',\n",
    "                \"nested\",all_images_DIP=\"Last\",global_it=-100, suffix=\"suffix\", last_iter=-1,ground_truth=ground_truth)\n",
    "    model_class = DIP_2D\n",
    "\n",
    "    # 设置log路径保存在runs目录\n",
    "    checkpoint_simple_path = 'test_logs'\n",
    "    name=str(datetime.datetime.now())\n",
    "\n",
    "    # 创建TensorBoardLogger，注意savedir和后面打开tensorboard时的路径要一致，versions是啥？\n",
    "    logger = pl.loggers.TensorBoardLogger(save_dir=checkpoint_simple_path,name=name)\n",
    "    print(config[\"sub_iter_DIP\"])\n",
    "    trainer = pl.Trainer(max_epochs=config[\"sub_iter_DIP\"],log_every_n_steps=100,logger=logger)#, callbacks=[checkpoint_callback, tuning_callback, early_stopping_callback], logger=logger,gpus=gpus, accelerator=accelerator, profiler=\"simple\")\n",
    "    trainer.fit(model, train_dataloader)\n",
    "    out = model(image_net_input_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
